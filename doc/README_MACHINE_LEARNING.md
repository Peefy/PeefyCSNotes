# 机器学习

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

* 传统机器学习：sklearn
* 深度学习：tensorflow theano pytorch
* 强化学习：
* 迁移学习：

有监督学习、无监督学习、判别模型、生成模型

[机器学习笔记](https://github.com/Peefy/StatisticalLearningMethod.Python/tree/master/src)

**1. 监督学习与非监督学习区别**

是否有监督（supervised），就看输入数据是否有标签（label）。输入数据有标签，则为有监督学习，没标签则为无监督学习。 

半监督学习：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。利用少量标注样本和大量未标注样本进行机器学习，从概率学习角度可理解为研究如何利用训练样本的输入边缘概率P(x)和条件输出概率P(y|x)的联系设计具有良好性能的分类器。

**2. L1范数和L2范数的区别**

L0范数是指向量中非0的元素的个数。(L0范数很难优化求解,非凸函数) 

L1范数是指向量中各个元素绝对值之和/n

L2范数是指向量各元素的平方和然后求平方根 

L2范数可以防止过拟合，提升模型的泛化能力，有助于处理 condition number不好下的矩阵(数据变化很小矩阵求解后结果变化很大) （核心：L2对大数，对outlier离群点更敏感！） 

下降速度：最小化权值参数L1比L2变化的快

模型空间的限制：L1会产生稀疏 L2不会。

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。

**3. 生成模型和判别模型区别**

监督学习分为生成模型和判别模型。 

生成模型：由数据学习联合概率分布，然后求出条件概率分布作为预测的模型。给定x产生出y的生成关系。e.g. 朴素贝叶斯、隐马尔科夫

判别模型：由数据直接学习决策函数或者条件概率分布作为预测模型。给定x应该预测什么样的输出y。e. g. K近邻算法(KNN)、感知机、决策树、逻辑斯蒂回归、最大熵、支持向量机(SVM)、提升方法、条件随机场

算法的优缺点以及相应解决方案：k-means, KNN, apriori 
算法原理：LR、KNN、k-means、apriori、ID3（C45,CART）、SVM、神经网络，协同过滤，em算法

**4. svm算法的原理、如何组织训练数据、如何调节惩罚因子、如何防止过拟合、svm的泛化能力、增量学习**

* **SVM算法的原理**-间隔最大的分离超平面 
* **如何组织训练数据**-min 2/|w| st yi(wi+b)>=1
* **如何调节惩罚因子**-拉格朗日乘子法算凸二次规划.对偶 max(a)min(w,b)L(w,b,a) 
w b  
* **如何防止过拟合**-f=sign(wx+b) 
C间隔(C>0)-间隔越大 错误分类惩罚大 间隔越小 对错误分类惩罚小 
调整C正则化特征
* **svm的泛化能力**-泛化能力强
* **增量学习**-

**5. 神经网络参数相关。比如，参数的范围？如何防止过拟合？隐藏层点的个数多了怎样少了怎样？什么情况下参数是负数？**

初始权重 -0.5-0.5 0-1 
隐藏节点个数 
传输函数 
学习速率(太大不稳定 太小太慢) 
减少隐藏节点个数 
多了过拟合 

**6. 为什么要用逻辑回归？**

逻辑回归的优点： 
1.实现简单； 
2.分类计算量小、速度快，存储资源低 
缺点： 
1、容易欠拟合，一般准确度不太高 
2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；

**7. 决策树算法是按什么来进行分类的?**

ID3 信息增益 
C4.5 信息增益率 
CART 基尼系数(二叉树)

**8. 朴素贝叶斯公式：**

P(Y|X)=P(X,Y)/P(X)=(P(X|Y)*P(Y))/P(X) –通过朴素贝叶斯条件独立展开 
P(A|B)=P(B|A)*P(A)/P(B) 
对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别

**9. svm中rbf核函数与高斯核函数的比较:**

在SVM的应用中，径向基函数就是指高斯核函数； 
exp(-(x-c)^2/r^2) r方差c均值 svm里面 c为xj

**10. 决策树分析：**

特征选择、生成树、剪枝 if-then规则

**11. SVM有哪些优势，（x,y,z）三个特征如何用径向基核函数抽取第四维特征**

SVM算法优点： 
* 可用于线性/非线性分类，也可以用于回归； 
* 低泛化误差； 
* 容易解释； 

缺点： 
* 对参数和核函数的选择比较敏感； 
* 原始的SVM只比较擅长处理二分类问题；

**12. 如何用Logic regression建立一个广告点击次数预测模型**

输入x 

用户特征 

人口属性如年龄、性别、收入;兴趣标签;历史点击率 

广告特征 

广告类别、广告id、广告主id、行业、素材图像特征等

上下文特征 

广告位、地域、时间、操作系统、浏览器等 

输出(h(x)) 

用户是否会点击某个广告(点击的概率是多大) 

特征处理-离散化-交叉特征-归一-onehot 

用户->从广告集合里规则抽取部分->ctr预估这部分广告

**13. 举一个适合采用层次分析法的例子**

目标->准则->方案 

构建对比较矩阵(某一准则比重)

排序计算方案权重 

买钢笔->价格外观质量->可供选择的笔

**14. 关联分析中的极大频繁项集；FP增长算法**

* 扫描数据库一遍，得到频繁项的集合F和每个频繁项的支持度。把F按支持度递降排序。 
* 构造原始FPTree 以null为根 顺序插入 
* 构建频繁项集。同一个频繁项在PF树中的所有节点的祖先路径的集合。 
比如I3在FP树中一共出现了3次，其祖先路径分别是{I2，I1：2(频度为2)}，{I2：2}和{I1：2}。这3个祖先路径的集合就是频繁项I3的条件模式基。

**15. 线性分类器与非线性分类器的区别及优劣**

线性分类器：模型是参数的线性函数，分类平面是（超）平面； 

非线性分类器：模型分界面可以是曲面或者超平面的组合。 可以解决线性不可分问题(异或问题) 
典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核）； 

典型的非线性分类器有kNN，决策树，SVM（非线性核） 

**16. 如何解决过拟合问题**

early stopping(设置epoch大小即计算多少次损失函数不再下降就认为达到最优)、数据集扩增（Data augmentation 可以在原始数据上做些改动，得到更多的数据）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout(神经网络里面每次随机删除一部分隐藏神经元)。

**17. 如何选取超参数学习速率、正则项系数、Mini-batch size**

学习速率（learning rate，η）： 

学习速率太小，则会使收敛过慢，如果学习速率太大，则会导致代价函数振荡。 

尝试：先把学习速率设置为0.01，然后观察training cost的走向，如果cost在减小，那你可以

逐步地调大学习速率，试试0.1，1.0….如果cost在增大，那就得减小学习速率，试试0.001，

0.0001….经过一番尝试之后，你可以大概确定学习速率的合适的值。 

出现early stopping的，但是我们可以不stop，而是让learning rate减半，之后让程序继续

跑。下一次validation accuracy又满足no-improvement-in-n规则时，我们同样再将

learning rate减半（此时变为原始learni rate的四分之一）…继续这个过程，直到learning

rate变为想要的结果再终止程序 

正则项系数（regularization parameter, λ） 

一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍（增减10倍是粗调节，当你确定了λ的合适的数量级后，比如λ = 0.01,再进一步地细调节，比如调节为0.02，0.03，0.009之类。） 

Mini-batch size 

将m个样本的梯度求均值，替代online learning方法中单个样本的梯度值 

**18. L1和L2正则的区别，如何选择L1和L2正则**

* L1是模型各个参数的绝对值之和。L2是模型各个参数的平方和的开方值。
* L1会趋向于产生少量的特征，而其他的特征都是0.因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵.L2会选择更多的特征，这些特征都会接近于0。最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0

**19. 随机森林的学习过程：**

随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出 

学习过程: 

现在有N个训练样本，每个样本的特征为M个，需要建K颗树 

1从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差） 

2从M个特征中取m个特征左右子集特征(m

*随机森林的自助抽样、袋外估计*

每次选n个样本，lim(1-1/n)^n=1/e=0.36

自助抽样：N总体有放回的抽取n(n

**20. 随机森林中的每一棵树是如何学习的**

决策树

**21. 随机森林学习算法中CART树的基尼指数是什么**

变量的不确定性

**22. k-mean shift的机制**

迭代   

初始K个中心点(第二次开始使用上次生成的中心点) 

map计算每个样本距离那个近 输出 中心 样本 

reduce 计算每个簇的新的中心点 满足停止条件停止 不满足输出 新中心点 

**23. 实现最小二乘法**

1/2(h(x)-y)求偏导

**24. Bagging与Boosting的区别**

Bagging(并行) 

从N样本中有放回的采样N个样本 

对这N个样本在全属性上建立分类器(CART,SVM) 

重复上面的步骤，建立m个分类器 

预测的时候使用投票的方法得到结果 

Boosting(串行) 

1. 通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。 
2. 通过加法模型将弱分类器进行线性组合 

区别： 
1. 样本选择上： 
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 
2. 样例权重： 
Bagging：使用均匀取样，每个样例的权重相等 
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 
3. 预测函数： 
Bagging：所有预测函数的权重相等。 
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 
4. 并行计算： 
Bagging：各个预测函数可以并行生成 
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

**25. 动态规划**

动态规划：待求解的问题分解为若干个子阶段(下一阶段需要上一阶段结果) 
初始状态→│决策１│→│决策２│→…→│决策ｎ│→结束状态 

* 问题的阶段
* 每个阶段的状态 
* 从前一个阶段转化到后一个阶段之间的递推关系

树结构 

```java
class TreeNode{ 
    int value; 
    TreeNode left; 
    TreeNode right; 
} 
```

链表结构：每个节点Node都有一个值val和指向下个节点的链接next 

```java
class Node { 
    int val; 
    Node next; 
    Node(int x) { 
        val = x; 
        next = null; 
    } 
}
```

**26. SVM原理-SVM核技巧原理，如何选择核函数**

泛化误差界的公式为： 

R(w)≤Remp(w)+Ф(n/h) 

公式中R(w)就是真实风险，Remp(w)就是经验风险(分类器在给定样本上的误差)，Ф(n/h)就是置信风险(多大程度上可以信任分类器在未知数据上分类的结果)。 

统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。

SVM正是这样一种努力最小化结构风险的算法。 

SVM算法要求的样本数是相对比较少的(小样本，并不是说样本的绝对数量少) 

非线性，是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现

硬间隔支持向量机(线性分类器) 

软间隔支持向量机(线性分类器) 

非线性支持向量机(核技巧与软间隔最大化) 

线性可分：线性核函数 

线性不可分：选择非线性核函数：多项式核函数、高斯核函数、拉普拉斯核函数、sigmoid核函数

**27. PageRank原理**

PageRank两个基本假设： 
* 数量假设：入链数量越多，那么这个页面越重要。 
* 质量假设：越是质量高的页面指向页面A，则页面A越重要。

利用以上两个假设，PageRank算法刚开始赋予每个网页相同的重要性得分，通过迭代递归计算来更新每个页面节点的PageRank得分，直到得分稳定为止。 
步骤如下： 

* 在初始阶段：网页通过链接关系构建起Web图，每个页面设置相同的PageRank值，通过若干轮的计算，会得到每个页面所获得的最终PageRank值。随着每一轮的计算进行，网页当前的PageRank值会不断得到更新。 
* 在一轮中更新页面PageRank得分的计算方法：在一轮更新页面PageRank得分的计算中，每个页面将其当前的PageRank值平均分配到本页面包含的出链上，这样每个链接即获得了相应的权值。而每个页面将所有指向本页面的入链所传入的权值求和，即可得到新的PageRank得分。当每个页面都获得了更新后的PageRank值，就完成了一轮PageRank计算。

优点： 
是一个与查询无关的静态算法，所有网页的PageRank值通过离线计算获得；有效减少在线查询时的计算量，极大降低了查询响应时间。

缺点： 
* 人们的查询具有主题特征，PageRank忽略了主题相关性，导致结果的相关性和主题性降低 
* 旧的页面等级会比新页面高。因为即使是非常好的新页面也不会有很多上游链接，除非它是某个站点的子站点。 

**28. AUC的定义和本质**

ROC曲线AUC为ROC曲线下的面积 越大分类越准 

横轴：负正类率FPR FP / (FP+TN=N) 直观解释：实际是0负中，错猜多少

纵轴：真正类率TPR TP / (TP+FN=P) 直观解释：实际是1正的中，猜对多少

auc的直观含义是任意取一个正样本和负样本，正样本得分大于负样本的概率。 

分类器能输出score： 
* 先把score排序一边扫描一边计算AUC近似的认为是一个一个矩形面积累加(阶梯状的)计算麻烦
* 统计一下所有的 M×N(M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少个组中的正样本的score大于负样本的score。当二元组中正负样本的 score相等的时候，按照0.5计算。然后除以MN。实现这个方法的复杂度为O(n^2)。n为样本数（即n=M+N） 
* 对score从大到小排序，最大score对应的sample 的rank为n，第二大score对应sample的rank为n-1

然后把所有的正类样本的rank相加，再减去正类样本的score为最小的那M个值的情况。得到的就是所有的样本中有多少对正类样本的score大于负类样本的score。然后再除以M×N。即 
AUC=((所有的正例位置相加)-(M*(M+1)/2))/(M*N) 

**29. 梯度提升树gbdt和xgboost区别**

传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 

传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 

xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 

Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 

列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 

xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

**30. 预处理，特征工程**

* 标准化 
* 数据归一化规范化Normalization 
* 特征二值化Binarization 
* 类别数据编码 OneHot 编码 
* 标签二值化 
* 类别编码 
* 生成多项式特征

**31. 融合框架原理，优缺点，bagging，stacking，boosting，为什么融合能提升效果**

个人理解是按照不同的思路来组合基础模型，在保证准确度的同时也提升了模型防止过拟合的能力。针对弱学习器(泛化能力弱)效果明显，个体学习器满足：1好而不同，具有多样性2不能太坏 

Boosting(串行-减少偏差) 

Bagging(并行-减少方差) 

Stacking 

不同模型之间有差异，体现不同表达能力

**32. 2G内存里找100TB数据的中位数**

KD树，聚类，hash 

散列分治：大文件散列映射多个小文件-小文件top-合并大文件top堆排序/快排 

找出5亿个int型数的中位数： 

首先将这5亿个int型数划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，根据统计结果就可以判断中位数落到哪个区域，并知道这个区域中的第几大数刚好是中位数。然后，

第二次扫描只统计落在这个区域中的那些数就可以了。

**33. 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好**

目的是它能够让它符合我们所做的假设，使我们能够在已有理论上对其分析 

LR更适合处理稀疏数据 

逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；(哑变量) 

特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

**34. 特征选择方法有哪些**

* 相关系数法 使用相关系数法，先要计算各个特征对目标值的相关系 
* 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征 
* 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性 
(分别使用L1和L2拟合，如果两个特征在L2中系数相接近，在L1中一个系数为0一个不为0，那么其实这两个特征都应该保留，原因是L1对于强相关特征只会保留一个) 
* 训练能够对特征打分的预选模型：RandomForest和LogisticRegression/GBDT等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
* 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见
* 通过深度学习来进行特征选择
* 传统用前进或者后退法的逐步回归来筛选特征或者对特征重要性排序，对于特征数量不多的情况还是适用的
* 方差选择法计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征
* 卡方检验 经典的卡方检验是检验定性自变量对定性因变量的相关性
* 互信息法 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的
* 线性判别分析法（LDA）
* 主成分分析法（PCA）

**35. AdaBoost, GBLT, XGBoost, LightGBM**

**36. 信息熵和基尼指数的关系**

信息熵在x = 1处一阶泰勒展开就是基尼指数

**37. 如何克服过拟合，欠拟合**

L0，L1，L2正则化(如果能推导绝对是加分项，一般人最多能画个等高线，L0是NP问题)

**38. 特征比数据量还大时，选择什么样的分类器**

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分

**39. 对于维度很高的特征，你是选择线性还是非线性分类器**

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分

**40. 对于维度极低的特征，你是选择线性还是非线性分类器**

非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分

**41. 正则化**

正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。
奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。

**42. 过拟合**

如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。

产生过拟合的原因：
* **样本数据的问题**-样本数量太少。抽样方法错误，抽出的样本数据不能有效足够代表业务逻辑或业务场景。比如样本符合正态分布，却按均分分布抽样，或者样本数据不能代表整体数据的分布，样本里的噪音数据干扰过大
* **模型问题**-模型复杂度高 、参数太多，决策树模型没有剪枝，权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.

解决方法
* **样本数据方面**-增加样本数量，对样本进行降维，添加验证数据,抽样方法要符合业务场景,清洗噪声数据.
* **模型或训练问题**-控制模型复杂度，优先选择简单的模型，或者用模型融合技术。利用先验知识，添加正则项。L1正则更加容易产生稀疏解、L2正则倾向于让参数w趋向于0. 交叉验证,不要过度训练，最优化求解时，收敛之前停止迭代。决策树模型没有剪枝,权值衰减

**43. 机器学习算法数据Feature**

* 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
* 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
* 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

**44. ill-condition病态问题**

训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题

**45. L1和L2正则的区别，如何选择L1和L2正则**

他们都是可以防止过拟合，降低模型复杂度

L1是在loss function后面加上 模型参数的1范数（也就是|xi|）

L2是在loss function后面加上 模型参数的2范数（也就是sigma(xi^2)），注意L2范数的定义是sqrt(sigma(xi^2))，在正则项上没有添加sqrt根号是为了更加容易优化

L1 会产生稀疏的特征

L2 会产生更多地特征但是都会接近于0

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。

**46. L1范数求解**

最小角回归算法：LARS算法

**47. 为什么越小的参数说明模型越简单**

过拟合的，拟合会经过曲面的每个点，也就是说在较小的区间里面可能会有较大的曲率，这里的导数就是很大，线性模型里面的权值就是导数，所以越小的参数说明模型越简单。

**48. 为什么一些机器学习模型需要对数据进行归一化？**

归一化化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。
1. 归一化后加快了梯度下降求最优解的速度。等高线变得显得圆滑，在梯度下降进行求解时能较快的收敛。如果不做归一化，梯度下降过程容易走之字，很难收敛甚至不能收敛
2. 把有量纲表达式变为无量纲表达式, 有可能提高精度。一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）
3. 逻辑回归等模型先验假设数据服从正态分布。

**49. 哪些机器学习算法不需要做归一化处理**

概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、gbdt、xgboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

**50. 特征向量的归一化方法**

* **线性函数转换**-y=(x-MinValue)/(MaxValue-MinValue)
* **对数函数转换**-y=log10(x)
* **反余切函数转换**-y=arctan(x)*2/PI
* **减去均值，乘以方差**-y=(x-means)/ variance

**51. 标准化与归一化的区别**

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

**52. 特征向量的缺失值处理**

* 缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。
* 缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:

1. 把NaN直接作为一个特征，假设用0表示；

2. 用均值填充；

3. 用随机森林等算法预测填充

**53. 随机森林如何处理缺失值**

* （na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。
* （rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。

**54. 随机森林如何评估特征重要性**

衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：
* Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。
* Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。

**55. 优化Kmeans**

使用kd树或者ball tree
将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可

**56. KMeans初始类簇中心点的选取**

k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。
* 从输入的数据点集合中随机选择一个点作为第一个聚类中心
* 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
* 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
* 重复2和3直到k个聚类中心被选出来
* 利用这k个初始的聚类中心来运行标准的k-means算法

**57. 解释对偶的概念**

一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

**58. 如何进行特征选择**

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解

常见的特征选择方式：

* 去除方差较小的特征
* 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
* 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
* 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。
```python
#correlation matrix
corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);
```
* 去除噪声点。通过matplotlib.scatter函数图示某一特征与预测特征的点分布图，明显看出噪声点，去除即可
```python
#bivariate analysis saleprice/grlivarea
var = 'GrLivArea'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));
 
df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]
df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)
df_train = df_train.drop(df_train[df_train['Id'] == 524].index)
```
* 标签编码，把字符类别特征编码成数值类型，如红绿蓝编码为0、1、2
* 归一化。将数据按比例缩放，使这些数据落入到一个较小的特定的区间之内。最小最大缩放;b.Z-score标准化。让数据服从基于 μ=0 和 σ=1的标准正态分布.
* OneHot编码get_dummies。有的离散特征如颜色需OneHot编码，编码后需特征对齐，因为进行one-hot编码后，会出现一种情况就是：某个特征的某一个取值只出现在训练集中，没有出现在测试集中，或者相反
* 产生K折交叉数据集：sklearn.cross_validation.KFold
* 填充均值前，通过对数运算修正数据基本符合正态分布

**59. 什么是偏差与方差？**

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

**60. 解决bias和Variance问题的方法**

交叉验证
* High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征
* High Variance解决方案：agging、简化模型、降维

**61. 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？**

用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

**62. SVM、LR、决策树的对比？**

* **模型复杂度**-SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝
* **损失函数**-SVM hinge loss; LR L2正则化; adaboost 指数损失
* **数据敏感度**-SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
* **数据量**-数据量大就用LR，数据量小且特征少就用SVM非线性核

**63. GBDT 和随机森林的区别**

随机森林采用的是bagging的思想，bagging又称为bootstrap aggreagation，通过在训练样本集中进行有放回的采样得到多个采样集，基于每个采样集训练出一个基学习器，再将基学习器结合。随机森林在对决策树进行bagging的基础上，在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性的时候是在当前节点属性集合中选择最优属性，而随机森林则是对结点先随机选择包含k个属性的子集，再选择最有属性，k作为一个参数控制了随机性的引入程度。

另外，GBDT训练是基于Boosting思想，每一迭代中根据错误更新样本权重，因此是串行生成的序列化方法，而随机森林是bagging的思想，因此是并行化方法。

**64. xgboost怎么给特征评分**

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。

```python

# feature importance
print(model.feature_importances_)
# plot
pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)
pyplot.show()
==========
# plot feature importance
plot_importance(model)
pyplot.show()
```

**65. 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？**

bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据oob（out of bag）,它可以用于取代测试集误差估计方法。

袋外数据(oob)误差的计算方法如下：

对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。

**66. 置信区间**

置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%。

**67. 监督学习一般使用两种类型的目标变量**

* **标称型**-标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类)
* **数值型**-数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)

**68. 为什么说朴素贝叶斯是高偏差低方差**

它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差

**69. 推荐算法**

* **内容关联算法(Content-Based)**-

CB算法的原理是将一个item的基本属性, 内容等信息提取出来, 抽成一个taglist, 为每个tag赋一个权重。

剩下的事情就跟一个搜索引擎非常类似了, 将所有item对应的taglist做一下倒排转换, 放到倒排索引服务器中存储起来。

当要对某一个item做相关推荐的时候, 将这个item对应的taglist拿出来拼成一个类似搜索系统中的query表达式, 再将召回的结果做一下排序作为推荐结果输出。

当要对某个用户做个性化推荐的时候, 将这个用户最近喜欢/操作过的item列表拿出来, 将这些item的taglist拿出来并merge一下作为用户模型, 并将这个模型的taglist请求倒排索引服务, 将召回的结果作为候选推荐给该用户。

该算法的好处为：
1. 不依赖于用户行为, 即不需要冷启动的过程, 随时到随时都能推荐
2. 可以给出看起来比较合理的推荐解释
3. item被推荐的时效性可以做得很高, 比如新闻类产品就需要用到该算法

该算法的坏处是:
1. 需要理解item的内容, 对音频/视频等不好解析内容的就不好处理了
2. 对于一次多义以及一义多词等情况处理起来比较复杂
3. 容易出现同质化严重的问题, 缺乏惊喜

* **协同过滤算法(Collaborative Filtering)**-

CF算法的原理是汇总所有`\<user,item\>`的行为对, 利用集体智慧做推荐。其原理很像朋友推荐, 比如通过对用户喜欢的item进行分析, 发现用户A和用户B很像(他们都喜欢差不多的东西), 用户B喜欢了某个item, 而用户A没有喜欢, 那么就把这个item推荐给用户A。(`User-Based CF`)

当然, 还有另外一个维度的协同推荐。即对比所有数据, 发现itemA和itemB很像(他们被差不多的人喜欢), 那么就把用户A喜欢的所有item, 将这些item类似的item列表拉出来, 作为被推荐候选推荐给用户A。(`Item-Based CF`)

如上说的都是个性化推荐, 如果是相关推荐, 就直接拿`Item-Based CF`的中间结果就好啦。

该算法的好处为：
1. 能起到意想不到的推荐效果, 经常能推荐出来一些惊喜结果
2. 进行有效的长尾item
3. 只依赖用户行为, 不需要对内容进行深入了解, 使用范围广

该算法的坏处是:
1. 一开始需要大量的`\<user,item\>`行为数据, 即需要大量冷启动数据
2. 很难给出合理的推荐解释

协同过滤算法具体实现的时候, 又分为典型的两类:
* 基于领域的协同过滤算法:

这类算法的主要思想是利用`\<user,item\>`的打分矩阵, 利用统计信息计算用户和用户, item和item之间的相似度。然后再利用相似度排序, 最终得出推荐结果。

该公式要计算用户i和用户j之间的相似度, I(ij)是代表用户i和用户j共同评价过的物品, R(i,x)代表用户i对物品x的评分, R(i)头上有一杠的代表用户i所有评分的平均分, 之所以要减去平均分是因为有的用户打分严有的松, 归一化用户打分避免相互影响。

该公式没有考虑到热门商品可能会被很多用户所喜欢, 所以还可以优化加一下权重, 这儿就不演示公式了。

在实际生产环境中, 经常用到另外一个类似的算法Slope One, 该公式是计算评分偏差, 即将共同评价过的物品, 将各自的打分相减再求平均。

Item-Based CF:这类算法会面临两个典型的问题:矩阵稀疏问题,计算资源有限导致的扩展性问题

基于此, 专家学者们又提出了系列基于模型的协同过滤算法
 
* 基于模型的协同过滤算法：基于模型的研究就多了, 常见的有:基于矩阵分解和潜在语义的；基于贝叶斯网络的；基于SVM的

3. 组合推荐技术。其实从实践中来看, 没有哪种推荐技术敢说自己没有弊端, 往往一个好的推荐系统也不是只用一种推荐技术就解决问题, 往往都是相互结合来弥补彼此的不足, 常见的组合方式如下:

* **混合推荐技术**-同时使用多种推荐技术再加权取最优;
* **切换推荐技术**-根据用户场景使用不同的推荐技术;
* **特征组合推荐技术**-将一种推荐技术的输出作为特征放到另一个推荐技术当中;
* **层叠推荐技术**-一个推荐模块过程中从另一个推荐模块中获取结果用于自己产出结果;

*Item-CF和User-CF选择*

* user和item数量分布以及变化频率
1. 如果user数量远远大于item数量, 采用Item-CF效果会更好, 因为同一个item对应的打分会比较多, 而且计算量会相对较少
2. 如果item数量远远大于user数量, 则采用User-CF效果会更好, 原因同上
3. 在实际生产环境中, 有可能因为用户无登陆, 而cookie信息又极不稳定, 导致只能使用item-cf
4. 如果用户行为变化频率很慢(比如小说), 用User-CF结果会比较稳定
5. 如果用户行为变化频率很快(比如新闻, 音乐, 电影等), 用Item-CF结果会比较稳定
* 相关和惊喜的权衡
1. item-based出的更偏相关结果, 出的可能都是看起来比较类似的结果
2. user-based出的更有可能有惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜
* 数据更新频率和时效性要求
1. 对于item更新时效性较高的产品, 比如新闻, 就无法直接采用item-based的CF, 因为CF是需要批量计算的, 在计算结果出来之前新的item是无法被推荐出来的, 导致数据时效性偏低;
2. 但是可以采用user-cf, 再记录一个在线的用户item行为对, 就可以根据用户最近类似的用户的行为进行时效性item推荐;
3. 对于像影视, 音乐之类的还是可以采用item-cf的;

**70. 数据降维方法-多维缩放MDS(Multiple Dimensional Scaling)**

有n个样本，每个样本的维度为m，目标是用不同的新的k维向量(k \<\< m)替代原来的n个m维向量，使得在新的低维空间中，所有样本相互之间的距离等于(或最大程度接近)原空间中的距离(默认欧式距离)。

**MDS算法**给出在给定k值条件下的最优解决方案，

* 计算所有原空间样本相互之间的距离平方矩阵Dist\[\]\[\]
* 根据Dist推算出目标降维后内积矩阵B

B\[i\]\[j\]=-0.5(Dist\[i\]\[j\] - avg(Dist\[i\]) - avg(Dist\[j\]) + avg_Dist)

* 对矩阵B做特征分解或者奇异值分解B=V\*diag\*V^T
* 取最大的k个特征值及其对应的特征向量构成diag_k和V_k,此时U=V_k\*diag_k就是降维后的n个行向量组成的矩阵。

```python
import numpy as np

# run this to get a test matrix
# A = np.random.randint(1,100,(5,20))
# np.save('mat.npy', A)
# exit()

A = np.load('mat.npy')

n,m = A.shape
Dist = np.zeros((n,n))
B = np.zeros((n,n))
for i in range(n):
    for j in range(n):
        Dist[i][j] = sum((ix-jx)**2 for ix,jx in zip(A[i], A[j]))

disti2 = np.array([0]*n)
distj2 = np.array([0]*n)

for x in range(n):
    disti2[x] = np.mean([Dist[x][j] for j in range(n)])
    distj2[x] = np.mean([Dist[i][x] for i in range(n)])

distij2 = np.mean([Dist[i][j] for i in range(n) for j in range(n)])

for i in range(n):
    for j in range(n):
        B[i][j] = -0.5*(Dist[i][j] - disti2[i] - distj2[j] + distij2)

w,v = np.linalg.eig(B)

v=v.transpose()

U = [{'eVal':w[i], 'eVec':v[i]} for i in range(n)]

U.sort(key = lambda obj:obj.get('eVal'), reverse = True)
k=4
w=np.array([0]*k)
v=np.zeros((k,n))

for i in range(k):
    w[i] = U[i].get('eVal')**0.5
    v[i] = U[i].get('eVec')

ans = np.dot(v.transpose(), np.diag(w))

ans_dist = np.zeros((n,n))
for i in range(n):
    ans_str=""
    for j in range(n):
        ans_dist[i][j] = sum((ix-jx)**2 for ix,jx in zip(ans[i], ans[j]))

print("Orign dis[][] is :")
print Dist
print("MDS dis[][] is :")
print(ans_dist)
```

**71. L-BFGS算法**

Broyden, Fletcher, Goldfarb, Shanno.四位数学家名字的首字母是BFGS，L是Limited memory的意思

L-BFGS是机器学习中解决函数最优化问题比较常用的手段，

BFGS算法是通过迭代逼近海森Hessian矩阵的逆的近似算法

$$x$$

**72. **

**73. **

**74. **

**75. **

**76. **

**77. **

**78. **

**79. **

**80. **

**81. **

**82. **

**83. **

**84. **

**85. **

**86. **

**87. **

**88. **

**89. **

**90. **

**91. **

**92. **

**93. **

**94. **

**95. **

**96. **

**97. **

**98. **

**99. **

**100. **

**101. **

**102. **

**103. **

**104. **

**105. **

**106. **

**107. **

**108. **

**109. **

**110. **

**111. **

**112. **

**113. **

**114. **

**115. **

**116. **

**117. **

**118. **

**119. **

**120. **

**121. **

**122. **

**123. **

**124. **

**125. **

**126. **

**127. **

**128. **

**129. **

**130. **

**131. **

**132. **

**133. **

**134. **

**135. **

**136. **

**137. **

**138. **

**139. **

**140. **

**141. **

**142. **

**143. **

**144. **

**145. **

**146. **

**147. **

**148. **

**149. **

**150. **

**151. **

**152. **

**153. **

**154. **

**155. **

**156. **

**157. **

**158. **

**159. **

**160. **

**161. **

**162. **

**163. **

**164. **

**165. **

**166. **

**167. **

**168. **

**169. **

**170. **

**171. **

**172. **

**173. **

**174. **

**175. **

**176. **

**177. **

**178. **

**179. **

**180. **

**181. **

**182. **

**183. **

**184. **

**185. **

**186. **

**187. **

**188. **

**189. **

**190. **

**191. **

**192. **

**193. **

**194. **

**195. **

**196. **

**197. **

**198. **

**199. **


